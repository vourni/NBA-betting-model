{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a737ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8-Model Stack (two LR, two XGB, GB, RF, Linear SVM+Calib, KNN)\n",
    "# Matches your existing cell style/format.\n",
    "# Expects: X_tr, y_tr, X_va, y_va, X_te, y_te already defined.\n",
    "# Uses your existing TwoStageStackTS class and prob_scorer.\n",
    "# ============================================================\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import log_loss, brier_score_loss, roc_auc_score, make_scorer, accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin, clone\n",
    "from sklearn.utils.validation import check_is_fitted\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "\n",
    "# ============================================================================\n",
    "# Custom Two-Stage Stacking Class\n",
    "# ============================================================================\n",
    "class TwoStageStackTS(BaseEstimator, ClassifierMixin):\n",
    "    _estimator_type = \"classifier\"\n",
    "    def __init__(self, base_estimators, meta_estimator, n_splits=5, gap=1, passthrough_idx=None):\n",
    "        self.base_estimators = base_estimators\n",
    "        self.meta_estimator = meta_estimator\n",
    "        self.n_splits = n_splits\n",
    "        self.gap = gap\n",
    "        self.passthrough_idx = passthrough_idx\n",
    "\n",
    "    def _oof(self, est, X, y, groups):\n",
    "        splitter = PurgedGroupTimeSeriesSplit(\n",
    "            n_splits=self.n_splits,\n",
    "            group_gap=self.gap\n",
    "        )\n",
    "        oof = np.full(len(X), np.nan, float)\n",
    "        for tr, te in splitter.split(X, y, groups=groups):\n",
    "            if len(tr) == 0 or len(te) ==0:\n",
    "                continue\n",
    "            e = clone(est).fit(X[tr], y[tr])\n",
    "            oof[te] = e.predict_proba(X[te])[:, 1]\n",
    "        return oof\n",
    "\n",
    "    def fit(self, X, y, groups=None):\n",
    "        if groups is None:\n",
    "            raise ValueError(\"TwoStageStackTS.fit requires 'groups' (e.g., normalized dates)\")\n",
    "        X = np.asarray(X); y = np.asarray(y)\n",
    "        groups = np.asarray(groups)\n",
    "        Zcols, self.base_fitted_ = [], []\n",
    "        for _, est in self.base_estimators:\n",
    "            Zcols.append(self._oof(est, X, y, groups))\n",
    "        Z = np.column_stack(Zcols)\n",
    "        mask = np.all(~np.isnan(Z), axis=1)\n",
    "        Z_tr, y_tr = Z[mask], y[mask]\n",
    "        if self.passthrough_idx is not None and len(self.passthrough_idx):\n",
    "            Z_tr = np.hstack([Z_tr, X[mask][:, self.passthrough_idx]])\n",
    "        self.meta_ = clone(self.meta_estimator).fit(Z_tr, y_tr)\n",
    "        for name, est in self.base_estimators:\n",
    "            self.base_fitted_.append((name, clone(est).fit(X, y)))\n",
    "        self.mask_ = mask\n",
    "        self.classes_ = np.array([0, 1])\n",
    "        return self\n",
    "\n",
    "    def _make_meta_X(self, X):\n",
    "        cols = [est.predict_proba(X)[:, 1] for _, est in self.base_fitted_]\n",
    "        Z = np.column_stack(cols)\n",
    "        if self.passthrough_idx is not None and len(self.passthrough_idx):\n",
    "            Z = np.hstack([Z, X[:, self.passthrough_idx]])\n",
    "        return Z\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        check_is_fitted(self, [\"meta_\", \"base_fitted_\"])\n",
    "        Z = self._make_meta_X(np.asarray(X))\n",
    "        return self.meta_.predict_proba(Z)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return (self.predict_proba(X)[:, 1] >= 0.5).astype(int)\n",
    "\n",
    "# ----------------------------------------------------------------\n",
    "# Scorer (same as before)\n",
    "# ----------------------------------------------------------------\n",
    "def probability_score(y_true, y_pred):\n",
    "    return 0.6 * log_loss(y_true, y_pred) + 0.4 * brier_score_loss(y_true, y_pred)\n",
    "\n",
    "prob_scorer = make_scorer(probability_score, response_method='predict_proba', greater_is_better=False)\n",
    "\n",
    "tss = PurgedGroupTimeSeriesSplit(n_splits=5, group_gap=1)\n",
    "\n",
    "# ============================================================================\n",
    "# BASE MODEL 1: Logistic Regression (Elastic Net)\n",
    "# ============================================================================\n",
    "print(\"=\" * 60)\n",
    "print(\"Training Logistic Regression (Elastic Net)...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "pipe_lr_elastic = make_pipeline(\n",
    "    StandardScaler(with_mean=True, with_std=True),\n",
    "    LogisticRegression(penalty=\"elasticnet\", solver=\"saga\", max_iter=10000, random_state=42)\n",
    ")\n",
    "\n",
    "lr_elastic_dist = {\n",
    "    \"logisticregression__C\": np.logspace(-4, 3, 80),\n",
    "    \"logisticregression__l1_ratio\": [0.1, 0.2, 0.3, 0.5, 0.7],\n",
    "    \"logisticregression__fit_intercept\": [True, False],\n",
    "    \"logisticregression__class_weight\": [None, \"balanced\"],\n",
    "}\n",
    "\n",
    "lr_elastic_search = RandomizedSearchCV(\n",
    "    estimator=pipe_lr_elastic,\n",
    "    param_distributions=lr_elastic_dist,\n",
    "    n_iter=60,\n",
    "    scoring=prob_scorer,\n",
    "    cv=tss,\n",
    "    n_jobs=-1,\n",
    "    refit=True,\n",
    "    random_state=42,\n",
    "    verbose=1\n",
    ")\n",
    "lr_elastic_search.fit(X_tr, y_tr, groups=groups_tr)\n",
    "print(f\"Best Elastic Net LR params: {lr_elastic_search.best_params_}\")\n",
    "print(f\"Best CV score: {-lr_elastic_search.best_score_:.4f}\")\n",
    "\n",
    "# ============================================================================\n",
    "# BASE MODEL 2: Logistic Regression (L2/Ridge)\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Training Logistic Regression (L2)...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "pipe_lr_l2 = make_pipeline(\n",
    "    StandardScaler(with_mean=True, with_std=True),\n",
    "    LogisticRegression(penalty=\"l2\", solver=\"lbfgs\", max_iter=10000, random_state=42)\n",
    ")\n",
    "\n",
    "lr_l2_dist = {\n",
    "    \"logisticregression__C\": np.logspace(-4, 4, 100),\n",
    "    \"logisticregression__fit_intercept\": [True, False],\n",
    "    \"logisticregression__class_weight\": [None, \"balanced\"],\n",
    "}\n",
    "\n",
    "lr_l2_search = RandomizedSearchCV(\n",
    "    estimator=pipe_lr_l2,\n",
    "    param_distributions=lr_l2_dist,\n",
    "    n_iter=50,\n",
    "    scoring=prob_scorer,\n",
    "    cv=tss,\n",
    "    n_jobs=-1,\n",
    "    refit=True,\n",
    "    random_state=42,\n",
    "    verbose=1\n",
    ")\n",
    "lr_l2_search.fit(X_tr, y_tr, groups=groups_tr)\n",
    "print(f\"Best L2 LR params: {lr_l2_search.best_params_}\")\n",
    "print(f\"Best CV score: {-lr_l2_search.best_score_:.4f}\")\n",
    "\n",
    "# ============================================================================\n",
    "# BASE MODEL 3: XGBoost (Shallow / probability-oriented)\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Training XGBoost (shallow)...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "xgb_shallow = xgb.XGBClassifier(\n",
    "    objective=\"binary:logistic\",\n",
    "    eval_metric=\"logloss\",\n",
    "    tree_method=\"hist\",\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "xgb_shallow_dist = {\n",
    "    \"max_depth\": [3, 4],\n",
    "    \"learning_rate\": [0.01, 0.02],\n",
    "    \"n_estimators\": [300, 500],\n",
    "    \"min_child_weight\": [3, 5, 7],\n",
    "    \"subsample\": [0.7, 0.85, 0.95],\n",
    "    \"colsample_bytree\": [0.6, 0.75, 0.9],\n",
    "    \"reg_lambda\": [1.0, 2.0, 5.0],\n",
    "    \"reg_alpha\": [0.0, 0.5, 1.0],\n",
    "}\n",
    "\n",
    "xgb_shallow_search = RandomizedSearchCV(\n",
    "    estimator=xgb_shallow,\n",
    "    param_distributions=xgb_shallow_dist,\n",
    "    n_iter=60,\n",
    "    scoring=prob_scorer,\n",
    "    cv=tss,\n",
    "    n_jobs=-1,\n",
    "    refit=True,\n",
    "    random_state=42,\n",
    "    verbose=1\n",
    ")\n",
    "xgb_shallow_search.fit(X_tr, y_tr, groups=groups_tr)\n",
    "print(f\"Best XGB (shallow) params: {xgb_shallow_search.best_params_}\")\n",
    "print(f\"Best CV score: {-xgb_shallow_search.best_score_:.4f}\")\n",
    "\n",
    "# ============================================================================\n",
    "# BASE MODEL 4: XGBoost (Deeper / stronger)\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Training XGBoost (deeper)...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "xgb_deep = xgb.XGBClassifier(\n",
    "    objective=\"binary:logistic\",\n",
    "    eval_metric=\"logloss\",\n",
    "    tree_method=\"hist\",\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "xgb_deep_dist = {\n",
    "    \"max_depth\": [5],\n",
    "    \"learning_rate\": [0.01, 0.015, 0.02],\n",
    "    \"n_estimators\": [600, 800],\n",
    "    \"min_child_weight\": [5, 7, 9],\n",
    "    \"subsample\": [0.7, 0.85],\n",
    "    \"colsample_bytree\": [0.6, 0.8],\n",
    "    \"reg_lambda\": [2.0, 5.0, 8.0],\n",
    "    \"reg_alpha\": [0.0, 0.5, 1.0],\n",
    "}\n",
    "\n",
    "xgb_deep_search = RandomizedSearchCV(\n",
    "    estimator=xgb_deep,\n",
    "    param_distributions=xgb_deep_dist,\n",
    "    n_iter=50,\n",
    "    scoring=prob_scorer,\n",
    "    cv=tss,\n",
    "    n_jobs=-1,\n",
    "    refit=True,\n",
    "    random_state=42,\n",
    "    verbose=1\n",
    ")\n",
    "xgb_deep_search.fit(X_tr, y_tr, groups=groups_tr)\n",
    "print(f\"Best XGB (deeper) params: {xgb_deep_search.best_params_}\")\n",
    "print(f\"Best CV score: {-xgb_deep_search.best_score_:.4f}\")\n",
    "\n",
    "# ============================================================================\n",
    "# BASE MODEL 5: Gradient Boosting (sklearn)\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Training Gradient Boosting (sklearn)...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "gb_base = GradientBoostingClassifier(random_state=42)\n",
    "\n",
    "gb_dist = {\n",
    "    \"learning_rate\": [0.01, 0.02, 0.03],\n",
    "    \"n_estimators\": [400, 600, 800],\n",
    "    \"max_depth\": [3, 4],\n",
    "    \"min_samples_leaf\": [10, 20],\n",
    "    \"subsample\": [0.7, 0.85, 1.0],\n",
    "}\n",
    "\n",
    "gb_search = RandomizedSearchCV(\n",
    "    estimator=gb_base,\n",
    "    param_distributions=gb_dist,\n",
    "    n_iter=60,\n",
    "    scoring=prob_scorer,\n",
    "    cv=tss,\n",
    "    n_jobs=-1,\n",
    "    refit=True,\n",
    "    random_state=42,\n",
    "    verbose=1\n",
    ")\n",
    "gb_search.fit(X_tr, y_tr, groups=groups_tr)\n",
    "print(f\"Best GB params: {gb_search.best_params_}\")\n",
    "print(f\"Best CV score: {-gb_search.best_score_:.4f}\")\n",
    "\n",
    "# ============================================================================\n",
    "# BASE MODEL 6: Random Forest (smoothed probs)\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Training Random Forest...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "rf_base = RandomForestClassifier(random_state=42, n_jobs=-1, oob_score=True)\n",
    "\n",
    "rf_dist = {\n",
    "    \"n_estimators\": [300, 400, 600],\n",
    "    \"max_depth\": [8, 10, 12],\n",
    "    \"min_samples_leaf\": [20, 30, 40],\n",
    "    \"min_samples_split\": [20, 30],\n",
    "    \"max_features\": ['sqrt', 0.5],\n",
    "    \"max_samples\": [0.7, 0.85, None],\n",
    "}\n",
    "\n",
    "rf_search = RandomizedSearchCV(\n",
    "    estimator=rf_base,\n",
    "    param_distributions=rf_dist,\n",
    "    n_iter=60,\n",
    "    scoring=prob_scorer,\n",
    "    cv=tss,\n",
    "    n_jobs=-1,\n",
    "    refit=True,\n",
    "    random_state=42,\n",
    "    verbose=1\n",
    ")\n",
    "rf_search.fit(X_tr, y_tr, groups=groups_tr)\n",
    "print(f\"Best RF params: {rf_search.best_params_}\")\n",
    "print(f\"Best CV score: {-rf_search.best_score_:.4f}\")\n",
    "\n",
    "# ============================================================================\n",
    "# BASE MODEL 7: Linear SVM (Calibrated)\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Training Linear SVM (Calibrated)...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Note: we wrap LinearSVC (no proba) in CalibratedClassifierCV for calibrated probabilities.\n",
    "svm_calib = CalibratedClassifierCV(\n",
    "    estimator=make_pipeline(\n",
    "        StandardScaler(with_mean=True, with_std=True),\n",
    "        LinearSVC(dual=\"auto\", max_iter=5000, random_state=42)\n",
    "    ),\n",
    "    method=\"sigmoid\",\n",
    "    cv=3\n",
    ")\n",
    "\n",
    "svm_dist = {\n",
    "    \"estimator__linearsvc__C\": np.logspace(-3, 2, 20),\n",
    "    \"method\": [\"sigmoid\", \"isotonic\"],\n",
    "}\n",
    "\n",
    "svm_search = RandomizedSearchCV(\n",
    "    estimator=svm_calib,\n",
    "    param_distributions=svm_dist,\n",
    "    n_iter=40,\n",
    "    scoring=prob_scorer,\n",
    "    cv=tss,\n",
    "    n_jobs=-1,\n",
    "    refit=True,\n",
    "    random_state=42,\n",
    "    verbose=1\n",
    ")\n",
    "svm_search.fit(X_tr, y_tr, groups=groups_tr)\n",
    "print(f\"Best SVM-Calibrated params: {svm_search.best_params_}\")\n",
    "print(f\"Best CV score: {-svm_search.best_score_:.4f}\")\n",
    "\n",
    "# ============================================================================\n",
    "# BASE MODEL 8: KNN (distance-weighted)\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Training KNN (distance-weighted)...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "pipe_knn = make_pipeline(\n",
    "    StandardScaler(with_mean=True, with_std=True),\n",
    "    KNeighborsClassifier(weights=\"distance\")\n",
    ")\n",
    "\n",
    "knn_dist = {\n",
    "    \"kneighborsclassifier__n_neighbors\": [25, 35, 50, 75, 100],\n",
    "    \"kneighborsclassifier__leaf_size\": [20, 30, 40],\n",
    "    \"kneighborsclassifier__p\": [1, 2],  # Manhattan / Euclidean\n",
    "}\n",
    "\n",
    "knn_search = RandomizedSearchCV(\n",
    "    estimator=pipe_knn,\n",
    "    param_distributions=knn_dist,\n",
    "    n_iter=40,\n",
    "    scoring=prob_scorer,\n",
    "    cv=tss,\n",
    "    n_jobs=-1,\n",
    "    refit=True,\n",
    "    random_state=42,\n",
    "    verbose=1\n",
    ")\n",
    "knn_search.fit(X_tr, y_tr, groups=groups_tr)\n",
    "print(f\"Best KNN params: {knn_search.best_params_}\")\n",
    "print(f\"Best CV score: {-knn_search.best_score_:.4f}\")\n",
    "\n",
    "# ============================================================================\n",
    "# Rebuild Final Models with Best Hyperparameters\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Building Final Base Models with Best Hyperparameters...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "lr_elastic_final = make_pipeline(StandardScaler(), LogisticRegression(penalty=\"elasticnet\", solver=\"saga\", max_iter=10000, random_state=42))\n",
    "lr_elastic_final.set_params(**lr_elastic_search.best_params_)\n",
    "\n",
    "lr_l2_final = make_pipeline(StandardScaler(), LogisticRegression(penalty=\"l2\", solver=\"lbfgs\", max_iter=10000, random_state=42))\n",
    "lr_l2_final.set_params(**lr_l2_search.best_params_)\n",
    "\n",
    "xgb_shallow_final = CalibratedClassifierCV(\n",
    "    estimator=xgb.XGBClassifier(objective=\"binary:logistic\", eval_metric=\"logloss\", tree_method=\"hist\", n_jobs=-1, random_state=42, **xgb_shallow_search.best_params_),\n",
    "    method='sigmoid',\n",
    "    cv=3\n",
    ") \n",
    "xgb_deep_final = CalibratedClassifierCV(\n",
    "    estimator=xgb.XGBClassifier(objective=\"binary:logistic\", eval_metric=\"logloss\", tree_method=\"hist\", n_jobs=-1, random_state=42, **xgb_deep_search.best_params_),\n",
    "    method='sigmoid',\n",
    "    cv=3\n",
    ") \n",
    "\n",
    "gb_final = CalibratedClassifierCV(\n",
    "    estimator=GradientBoostingClassifier(random_state=42, **gb_search.best_params_),\n",
    "    method='sigmoid',\n",
    "    cv=3\n",
    ")\n",
    "\n",
    "rf_final = CalibratedClassifierCV(\n",
    "    estimator=RandomForestClassifier(random_state=42, n_jobs=-1, oob_score=True, **rf_search.best_params_),\n",
    "    method='sigmoid',\n",
    "    cv=3\n",
    ")\n",
    "\n",
    "# For SVM we reuse the fitted configuration (CalibratedClassifierCV with best params)\n",
    "svm_final = CalibratedClassifierCV(\n",
    "    estimator=make_pipeline(\n",
    "        StandardScaler(),\n",
    "        LinearSVC(dual=\"auto\", max_iter=5000, random_state=42)\n",
    "    ),\n",
    "    method=svm_search.best_params_[\"method\"],\n",
    "    cv=3\n",
    ")\n",
    "# set best C on the inner LinearSVC\n",
    "svm_final.estimator.set_params(linearsvc__C=svm_search.best_params_[\"estimator__linearsvc__C\"])\n",
    "\n",
    "knn_final = make_pipeline(StandardScaler(), KNeighborsClassifier(weights=\"distance\"))\n",
    "knn_final.set_params(**{\n",
    "    k: v\n",
    "    for k, v in knn_search.best_params_.items()\n",
    "    if k.startswith(\"kneighborsclassifier__\")\n",
    "})\n",
    "\n",
    "# ============================================================================\n",
    "# Create Base Models List for Stacking\n",
    "# ============================================================================\n",
    "base_models = [\n",
    "    (\"lr_elastic\",  lr_elastic_final),\n",
    "    (\"lr_l2\",       lr_l2_final),\n",
    "    (\"xgb_shallow\", xgb_shallow_final),\n",
    "    (\"xgb_deep\",    xgb_deep_final),\n",
    "    (\"gb\",          gb_final),\n",
    "    (\"rf\",          rf_final),\n",
    "    (\"svm_linear\",  svm_final),\n",
    "    (\"knn\",         knn_final),\n",
    "]\n",
    "\n",
    "print(\"\\nBase models ready for stacking!\")\n",
    "print(f\"Total base models: {len(base_models)}\")\n",
    "\n",
    "# ============================================================================\n",
    "# Meta-Learner (Elastic-Net Logistic on OOF)\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Creating Meta-Learner...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "meta = make_pipeline(\n",
    "    StandardScaler(with_mean=True, with_std=True),\n",
    "    LogisticRegression(\n",
    "        solver=\"saga\",\n",
    "        penalty=\"elasticnet\",\n",
    "        l1_ratio=0.15,\n",
    "        C=0.1,\n",
    "        max_iter=5000,\n",
    "        tol=1e-3,\n",
    "        random_state=42\n",
    "    )\n",
    ")\n",
    "\n",
    "# ============================================================================\n",
    "# Fit Stacked Ensemble\n",
    "# ============================================================================\n",
    "stack = TwoStageStackTS(\n",
    "    base_estimators=base_models,\n",
    "    meta_estimator=meta,\n",
    "    n_splits=5,\n",
    "    gap=5,\n",
    "    passthrough_idx=None\n",
    ")\n",
    "\n",
    "print(\"\\nFitting stacked ensemble...\")\n",
    "stack.fit(X_tr, y_tr, groups=groups_tr)\n",
    "\n",
    "# ============================================================================\n",
    "# Evaluate\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Validation Results\")\n",
    "print(\"=\" * 60)\n",
    "p_va = stack.predict_proba(X_va)[:, 1]\n",
    "yhat = (p_va >= 0.5).astype(int)\n",
    "\n",
    "print(f\"Val Log Loss:  {log_loss(y_va, p_va):.4f}\")\n",
    "print(f\"Val Brier:     {brier_score_loss(y_va, p_va):.4f}\")\n",
    "print(f\"Val AUC:       {roc_auc_score(y_va, p_va):.4f}\")\n",
    "print(f\"Val Accuracy:  {accuracy_score(y_va, yhat):.4f}\")\n",
    "\n",
    "from sklearn.calibration import calibration_curve\n",
    "prob_true, prob_pred = calibration_curve(y_va, p_va, n_bins=10, strategy='quantile')\n",
    "ece = np.mean(np.abs(prob_true - prob_pred))\n",
    "print(f\"Val ECE:       {ece:.4f}\")\n",
    "\n",
    "model = stack\n",
    "print(\"\\nâœ“ Model training complete!\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
